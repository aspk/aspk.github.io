---
layout: post
title: Duplicate question detection in question pairs!
---
In this post, I tackle the problem of classifying questions pairs based on whether they are duplicate or not duplicate. This is important for companies like Quora, or Stack Overflow where multiple questions posted are duplicates of questions already answered. If a duplicate question is spotted by an algorithm, the user can be directed to it and reach the answer faster.

An example of two duplicate questions is 'How do I read and find my YouTube comments?' and 'How can I see all my Youtube comments?', and non duplicate questions is 'What's causing someone to be jealous?' and 'What can I do to avoid being jealous of someone?'.
Two approaches are applied to this problem:

1. Sequence Encoder trained by auto-encoder approach and dynamic pooling for classification
2. Bag of Words model with Logistic Regression and XGBoost classifier

Bag of words model with optimized ngrams and min_df achieves an accureacy of .. % as compared to 89.5% whicch is the best accuracies reported in literature with Bi LSTM and attention[1]. The encoder approach implemented here achieves 63.8% accuracy, which is lower than the other approaches. I found it interesting because of the autoencoder implementation and the approach considers similary between phrases as well as the words for variable length sequences. Perhaphs, the efficiency could be improved by changing the dimentions of dynamically pooled matrix, a different approach in cleaning the data, as well as spelling checks. 

Classifier's can be compared based on three different evaluation metrics, log loss, auc, and accuracy. Log loss or the cross entropy loss is an indicator of how different the probability distribution of the output of the classifier is relative to the true probability distribution of the class labels [2].  Receiver operating characteristic plots the true positive rate vs the false positive rate and an area under the curve (auc) of 0.5 corresponds to a random classifier. Higher the AUC better the classifier. Accuracy is a simple metric, which calculates the fraction of correct predicted labels.

In this post, I use accuracy as a metric for comparison, as there is specific reason to do otherwise. 

### BOW model
![](https://i.imgur.com/ljgkLpJ.png)


As shown in the figure,
For logistic regression , plot accuracy vs n grams and vs min_df
5 data points

The measured accuracy of the bag of words model is 77% for minimum data frequency of 10 and it changes from 75% to 77% when ngrams are changed from 1 to 3. Increasing ngrams from 2 to three shows minimal change in accuracy of the model.When the same procedure is used in a XGBoost classifier, the efficiency increases to 80.5%.

Increasing the the min_df decreases the accuracy for logistic regression, as the number of features increase. Higher min_df corresponds to a higher cutoff on the features considered filtered according to their count in the corpus. Higher n-grams corresponds to a higher accuracy and the change in accuracy is more drastic for logistic regression.

For XGBoost, for the best parameters, (n-grams = 3, min_df =10), this corresponds to a vocabulary size of 109050.



### Auto-encoder and Dynamic Pooling CNN classifier

![](https://i.imgur.com/XLKpKab.png)

As shown in the figure, the model considered is similar to Socher et al.[3]. Word2Vec embedding is generated with a vocabulary size of 100000 according to [4], using the skip gram model. In these embeddings, words which share similar context have smaller cosine distance. The key problem is dealing with questions of different lengths. The information content of a sentence is compressed by training an auto encoder. The main motivation behind this approach is to find similarity between sentences by comparing the entire sentence as well as the phrases in the sentence. The problem of different lengths is circumvented by upsampling and dynamic pooling as described below.

![](https://i.imgur.com/59WshFu.png)

Sentences are encoded using the approach shown in the left figure. The three words and the 2 encodings are considered as input to generate the similarity matrix. The auto-encoder is trained as shown in the right figure using Tensorflow [5]. In this post, I used a single layer NN for the encoder and the decoder, multiple hidden layers could also be considered. Multiple batches of words are concatenated and fed into the encoder and in the ideal case the output of the decoder should match the input. The loss of the neural net is minimized with Gradient Descent optimizer with learning rate of 0.1. L2 regularization coefficient of 1e-4 is used for the encoder and decoder weights. This approach is a slight simplification of the approach used by Socher et al. [3] , as I do not use encode the entire sentence and unfold it into a question while training. This is because of difficulty of implementation with tensorflow. Dynamic graph construction tools like pytorch could potentially be a better fit to implment the full approach. 

After the sentences are converted into embeddings, and a vector of encodings of phrases and the entire sentence is generated using the trained encoder. In the upsampling phase, the smaller vector of the question pair considered is upsampled by repeating the encodings randomly chosen of the vector to match the maximum length of the other sentence. A pairwise similarity matrix is generated for each phrase vector , and the variable dimention matrix is pooled into a matrix of npool x npool. I used npool = 28. This matrix is fed into a CNN classifier to classify as duplicate or not. A hyper parameter optimization of npool could also increase tha accuracy. The accuracy of this model is 63.8 % .


References:
[1] Wang, Zhiguo, Wael Hamza, and Radu Florian. "Bilateral multi-perspective matching for natural language sentences." arXiv preprint arXiv:1702.03814 (2017).
[2] http://colah.github.io/posts/2015-09-Visual-Information/
[3] Socher, Richard, et al. "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection." Advances in neural information processing systems. 2011.
[4]https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py
[5] https://www.tensorflow.org/

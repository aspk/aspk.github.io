---
layout: post
title: Duplicate question detection in question pairs!
---
In this post, I tackle the problem of classifying questions pairs based on whether they are duplicate or not duplicate.

An example of two duplicate questions is 'How do I read and find my YouTube comments?' and 'How can I see all my Youtube comments?', and non duplicate questions is 'What's causing someone to be jealous?' and 'What can I do to avoid being jealous of someone?'.
Two approaches are applied to this problem:

1. Bag of Words model with Logistic Regression and XGBoost classifier

2. Auto-encoder and Dynamic Pooling for classification [1]

Classifier's can be compared based on three different evaluation metrics. Log loss or the cross entropy loss is an indicator of how different the probability distribution of the output of the classifier is relative to the true probability distribution of the class labels [2].  Receiver operating characteristic plots the true positive rate vs the false positive rate and an area under the curve (auc) of 0.5 corresponds to a random classifier. Higher the AUC better the classifier. Accuracy is a simple metric, which calculates the fraction of correct predicted labels.

In this post, I use accuracy as a metric for comparison.

###BOW model
As shown in the figure,
For logistic regression , plot accuracy vs n grams and vs min_df
5 data points

The measured accuracy of the bag of words model is 77% for minimum data frequency of 10 and it changes from 75% to 77% when ngrams are changed from 1 to 3. Increasing ngrams from 2 to three shows minimal change in accuracy of the model.When the same procedure is used in a XGBoost classifier, the efficiency increases to 80.5%.

Increasing the the min_df decreases the accuracy for logistic regression, as the number of features increase. Higher min_df corresponds to a higher cutoff on the features considered filtered according to their count in the corpus. Higher n-grams corresponds to a higher accuracy and the change in accuracy is more drastic for logistic regression.

For XGBoost, for the best parameters, (n-grams = 3, min_df =10), this corresponds to a vocabulary size of 109050.


###Auto-encoder and Dynamic Pooling CNN classifier

As shown in the figure, the model considered is similar to [1]. Word2Vec embedding is generated with a vocabulary size of 100000 according to [2], using the skip gram model. In these embeddings, words which share similar context have smaller cosine distance. The key problem is dealing with sentences of different lengths. The information content of a sentence is compressed by training an auto encoder.

After the sentences are converted into embeddings, the smaller sentence is upsampled by repeating the words in the sentence to match the maximum length of the other sentence.

A pairwise similarity matrix is calculated for each phrase vector and this matrix is used in a CNN classifier to classify as duplicate or not.

The accuracy of this model is .. %.


References:

[1] Socher, Richard, et al. "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection." Advances in neural information processing systems. 2011.
[2] http://colah.github.io/posts/2015-09-Visual-Information/
[3]

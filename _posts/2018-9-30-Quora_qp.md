---
layout: post
title: Duplicate question detection in question pairs!
---
In this post, I tackle the problem of classifying questions pairs based on whether they are duplicate or not duplicate. This is important for companies like Quora, or Stack Overflow where multiple questions posted are duplicates of questions already answered. If a duplicate question is spotted by an algorithm, the user can be directed to it and reach the answer faster.

An example of two duplicate questions is 'How do I read and find my YouTube comments?' and 'How can I see all my Youtube comments?', and non duplicate questions is 'What's causing someone to be jealous?' and 'What can I do to avoid being jealous of someone?'.
Two approaches are applied to this problem:

1. Sequence Encoder trained by auto-encoder approach and dynamic pooling for classification
2. Bag of Words model with Logistic Regression and XGBoost classifier

Classifier's can be compared based on three different evaluation metrics, Log loss, auc, and accuracy. Log loss or the cross entropy loss is an indicator of how different the probability distribution of the output of the classifier is relative to the true probability distribution of the class labels [1].  Receiver operating characteristic plots the true positive rate vs the false positive rate and an area under the curve (auc) of 0.5 corresponds to a random classifier. Higher the AUC better the classifier. Accuracy is a simple metric, which calculates the fraction of correct predicted labels.

In this post, I use accuracy as a metric for comparison.

### Auto-encoder and Dynamic Pooling CNN classifier
![](https://i.imgur.com/XLKpKab.png)


As shown in the figure, the model considered is similar to Socher et al.[2]. Word2Vec embedding is generated with a vocabulary size of 100000 according to [3], using the skip gram model. In these embeddings, words which share similar context have smaller cosine distance. The key problem is dealing with sentences of different lengths. The information content of a sentence is compressed by training an auto encoder. The main motivation behind this approach is to find similarity between sentences by comparing the entire sentence as well as the phrases in the sentence. The problem of different lengths is circumvented by upsampling and dynamic pooling.
![](https://i.imgur.com/59WshFu.png)


The auto-encoder is trained as shown in the figure using Tensorflow [4]. In this post, I used a single layer NN for the encoder and the decoder. Multiple batches of words are concatenated and fed into the encoder and in the ideal case the output of the decoder should match the input. The loss of the neural net is minimized with Gradient Descent optimizer with learning rate of 0.1. L2 regularization coefficient of 1e-4 is used for the encoder and decoder weights.  

After the sentences are converted into embeddings, and a vector of encodings of phrases and the entire sentence is generated using the trained encoder. The smaller vector of the question pair considered is upsampled by repeating the encodings randomly chosen of the vector to match the maximum length of the other sentence. A pairwise similarity matrix is generated for each phrase vector and this matrix is used in a CNN classifier to classify as duplicate or not.

The accuracy of this model is .. % .


### BOW model
As shown in the figure,
For logistic regression , plot accuracy vs n grams and vs min_df
5 data points

The measured accuracy of the bag of words model is 77% for minimum data frequency of 10 and it changes from 75% to 77% when ngrams are changed from 1 to 3. Increasing ngrams from 2 to three shows minimal change in accuracy of the model.When the same procedure is used in a XGBoost classifier, the efficiency increases to 80.5%.

Increasing the the min_df decreases the accuracy for logistic regression, as the number of features increase. Higher min_df corresponds to a higher cutoff on the features considered filtered according to their count in the corpus. Higher n-grams corresponds to a higher accuracy and the change in accuracy is more drastic for logistic regression.

For XGBoost, for the best parameters, (n-grams = 3, min_df =10), this corresponds to a vocabulary size of 109050.





References:

[1] http://colah.github.io/posts/2015-09-Visual-Information/
[2] Socher, Richard, et al. "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection." Advances in neural information processing systems. 2011.
[3]https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/examples/tutorials/word2vec/word2vec_basic.py
[4] https://www.tensorflow.org/
